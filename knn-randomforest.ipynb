{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.load(os.path.join(DATA_DIR,\"processed_sents_dl.npy\"),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = np.load(os.path.join(DATA_DIR,\"processed_tags_dl.npy\"),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The idea behind using KNN and Random forests is as follows:\n",
    "We have a BIO tag for each word in the data given. So I used the words I obtained from the cleaned data and used the glove \n",
    "embedding to represent each word I had. The I attempt to create a KNN or random forest classifier using the BIO tags and \n",
    "word embeddings. The idea being if two words are similar they have similar embeddings and hence similar tags. I got this idea\n",
    "from the word cloud I created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading glove embeddings to a dictionary embeddings_index\n",
    "embeddings_index = {}\n",
    "f = open(\"../embeddings/glove.6B.50d.txt\", encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.strip().split(' ')\n",
    "    word = values[0] # the first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') #50d vectors   \n",
    "    #representing the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the data set with X representing the corresponding word embedding for every word in the cleaned data\n",
    "#y are the BIO tags\n",
    "X=[]\n",
    "y=[]\n",
    "for i,sent in enumerate(sentences):\n",
    "    tag=tags[i]\n",
    "    for j,word in enumerate(sent):\n",
    "        currentTag = tag[j]\n",
    "        y.append(currentTag)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            X.append(embedding_vector)\n",
    "        else:\n",
    "            X.append([0]*EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y= np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and valiation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34952, 50)\n",
      "(11651, 50)\n",
      "(34952,)\n",
      "(11651,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the data to knn classifier and obtain predictions\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "y_pred = neigh.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is no point in evaluating on all classes since 'O' is the dominating class and its F1 score is always going to be high\n",
    "#evaluate on all classes except 'O'\n",
    "all_classes = list(np.unique(y_train))\n",
    "\n",
    "new_classes = all_classes.copy()\n",
    "\n",
    "new_classes.remove('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHIT\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_val, y_pred, digits=4,labels=new_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  B-corporation     0.6786    0.3878    0.4935        49\n",
      "B-creative-work     0.3333    0.0263    0.0488        38\n",
      "        B-group     0.2857    0.1754    0.2174        57\n",
      "     B-location     0.5000    0.3937    0.4405       127\n",
      "       B-person     0.6043    0.5153    0.5563       163\n",
      "      B-product     0.3462    0.2903    0.3158        31\n",
      "  I-corporation     0.0000    0.0000    0.0000        12\n",
      "I-creative-work     0.1111    0.0185    0.0317        54\n",
      "        I-group     0.0000    0.0000    0.0000        27\n",
      "     I-location     0.3704    0.1538    0.2174        65\n",
      "       I-person     0.3538    0.2875    0.3172        80\n",
      "      I-product     0.5000    0.0476    0.0870        42\n",
      "\n",
      "      micro avg     0.4772    0.2805    0.3533       745\n",
      "      macro avg     0.3403    0.1914    0.2271       745\n",
      "   weighted avg     0.4219    0.2805    0.3218       745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#knn doesn't do very well. It has an F1 score of just 0.267\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this model\n",
    "import pickle\n",
    "filename = '../models/knn.sav'\n",
    "pickle.dump(neigh, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a random forest classifier to the data\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHIT\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_val, y_pred, digits=4,labels=new_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  B-corporation     0.7500    0.4898    0.5926        49\n",
      "B-creative-work     0.1111    0.0263    0.0426        38\n",
      "        B-group     0.7222    0.2281    0.3467        57\n",
      "     B-location     0.6250    0.3150    0.4188       127\n",
      "       B-person     0.7010    0.4172    0.5231       163\n",
      "      B-product     0.8889    0.2581    0.4000        31\n",
      "  I-corporation     0.0000    0.0000    0.0000        12\n",
      "I-creative-work     0.2727    0.0556    0.0923        54\n",
      "        I-group     0.0000    0.0000    0.0000        27\n",
      "     I-location     0.3000    0.1385    0.1895        65\n",
      "       I-person     0.6250    0.2500    0.3571        80\n",
      "      I-product     0.2632    0.1190    0.1639        42\n",
      "\n",
      "      micro avg     0.5913    0.2564    0.3577       745\n",
      "      macro avg     0.4383    0.1915    0.2605       745\n",
      "   weighted avg     0.5351    0.2564    0.3410       745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this does much better than knn since it is an ensemble technique which can handle the problem of class imbalance much better.\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this model\n",
    "import pickle\n",
    "filename = '../models/random-forest.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
