{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFile = os.path.join(DATA_DIR,\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#I noticed an empty line in the text file denotes end of a sentence. So I am reading the file and \n",
    "#returning all sentences present in the file.\n",
    "\n",
    "def read_file(filename):\n",
    "    #open the file\n",
    "    fpt = open(filename,\"r\",encoding=\"utf8\")\n",
    "    \n",
    "\n",
    "    #store the current sentence in this list\n",
    "    currentSentence=[]\n",
    "    #list to store all sentences in the file\n",
    "    allSentences=[]\n",
    "    for line in fpt:\n",
    "        \n",
    "        lineSplit = line.split()\n",
    "        \n",
    "        #if line is empty it indicates end of the sentence. Append the current sentence to allSentences and reinitialize\n",
    "        #currentSentence to an empty list\n",
    "        if len(lineSplit)==0:\n",
    "            allSentences.append(currentSentence)\n",
    "            currentSentence=[]\n",
    "        #else add the word to the corresponding BIO label in the dictionary and add the word to the current sentence\n",
    "        else:\n",
    "            word = lineSplit[0]\n",
    "            currentSentence.append(word)\n",
    "    \n",
    "\n",
    "    fpt.close()\n",
    "    \n",
    "    return allSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = read_file(testFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['&', 'gt', ';', '*', 'The', 'soldier', 'was', 'killed', 'when', 'another', 'avalanche', 'hit', 'an', 'army', 'barracks', 'in', 'the', 'northern', 'area', 'of', 'Sonmarg', ',', 'said', 'a', 'military', 'spokesman', '.']\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the sentences as we had done for training. However in this case any words like urls, twitter @s or punctuations\n",
    "#in the sentence are marked as <UNK> or unknown because at test time we need a prediction for every word. \n",
    "#These will be predicted as 'O'. Also make all words lower case\n",
    "def pre_process(sentence):\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        word = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word)\n",
    "        word = re.sub('@[^\\s]+','',word)\n",
    "        if word.isalpha() and len(word)>0:\n",
    "            result.append(word.lower())\n",
    "        else:\n",
    "            result.append(\"<UNK>\")\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,sentence in enumerate(test_sentences):\n",
    "\n",
    "    processedSen = pre_process(sentence)\n",
    "    test_sentences[i]=processedSen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<UNK>', 'gt', '<UNK>', '<UNK>', 'the', 'soldier', 'was', 'killed', 'when', 'another', 'avalanche', 'hit', 'an', 'army', 'barracks', 'in', 'the', 'northern', 'area', 'of', 'sonmarg', '<UNK>', 'said', 'a', 'military', 'spokesman', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot sentence by length\n",
    "plt.hist([len(s) for s in test_sentences], bins=50)\n",
    "plt.title('Number of words per sentence')\n",
    "plt.xlabel('Len (sentence)')\n",
    "plt.ylabel('# samples')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the word2Idx,idx2Word,label2Idx and idx2Label dictionaries we had created on training data\n",
    "#use it to convert the test sentences to list of indices as expected by a Bi-LSTM model\n",
    "with open('../data/word2Idx.pickle', 'rb') as handle:\n",
    "    word2Idx = pickle.load(handle)\n",
    "    \n",
    "with open('../data/idx2Word.pickle', 'rb') as handle:\n",
    "    idx2Word = pickle.load(handle)\n",
    "\n",
    "with open('../data/label2Idx.pickle', 'rb') as handle:\n",
    "    label2Idx = pickle.load(handle)\n",
    "    \n",
    "with open('../data/idx2Label.pickle', 'rb') as handle:\n",
    "    idx2Label = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to map the sentence to their correspond ids from the dictionary created above\n",
    "def map_to_idx(sentence):\n",
    "    sentenceToIdx=[]\n",
    "    \n",
    "    for i,word in enumerate(sentence):\n",
    "        if word in word2Idx:\n",
    "            wordIdx = word2Idx[word]\n",
    "        elif word.lower() in word2Idx:\n",
    "            wordIdx = word2Idx[word.lower()]\n",
    "        else:\n",
    "            wordIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "\n",
    "        sentenceToIdx.append(wordIdx)\n",
    "        \n",
    "    return sentenceToIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataset by mapping words and tags to the ids\n",
    "X_test = []\n",
    "samples=len(test_sentences)\n",
    "for i in range(samples):\n",
    "    \n",
    "    sentence=test_sentences[i]\n",
    "    sentenceToIdx = map_to_idx(sentence)\n",
    "    X_test.append(sentenceToIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the above graph I decided setting the max sentence length to 105 is a good idea\n",
    "#pad all sentences to the same length of 105\n",
    "\n",
    "def padding(sentences, max_len, padding='post'):\n",
    "    padded_sentences = pad_sequences(sentences, max_len,padding='post')\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "X_test = padding(X_test, 105, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', '<UNK>', 'kiiingpin', '<UNK>', '<UNK>', 'i', 'want', 'to', 'be', 'someone', '<UNK>', 's', 'favorite', 'person', 'to', 'talk', 'to']\n",
      "['rt', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'i', 'want', 'to', 'be', 'someone', 'UNKNOWN_TOKEN', 's', 'favorite', 'person', 'to', 'talk', 'to', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN']\n"
     ]
    }
   ],
   "source": [
    "#sanity check that we have the done the mapping right and created the right dataset\n",
    "i=1200\n",
    "#print(X[i])\n",
    "#print(y[i])\n",
    "print(test_sentences[i])\n",
    "\n",
    "print([idx2Word[w] for w in X_test[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1287, 105)\n"
     ]
    }
   ],
   "source": [
    "#number of sentences, length of each sentence\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=100\n",
    "BATCH = 32\n",
    "#set from the graph above\n",
    "max_seq_len = 105\n",
    "num_labels = len(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading glove embeddings\n",
    "embeddings_index = {}\n",
    "f = open(\"../embeddings/glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.strip().split(' ')\n",
    "    word = values[0] # the first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') #50d vectors   \n",
    "    #representing the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2Idx), EMBEDDING_DIM))\n",
    "\n",
    "# Word embeddings for the tokens\n",
    "for word,i in word2Idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.batch(BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model which has the following architecture\n",
    "#Embedding->BiLSTM->Dense\n",
    "class NERmodel(tf.keras.Model):\n",
    "    def __init__(self, max_seq_len, embed_input_dim, embed_output_dim, num_labels, weights):\n",
    "\n",
    "        super(NERmodel, self).__init__() \n",
    "        self.embedding = layers.Embedding(input_dim=embed_input_dim,\n",
    "                                          output_dim=embed_output_dim, weights=weights,\n",
    "                                          input_length=max_seq_len, trainable=True, mask_zero=True)        \n",
    "\n",
    "        self.bilstm = layers.Bidirectional(layers.LSTM(512,return_sequences=True))\n",
    "    \n",
    "        self.dense = layers.Dense(num_labels)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.embedding(inputs) # batchsize, max_seq_len,embedding_output_dim\n",
    "        x = self.bilstm(x) #batchsize, max_seq_len, hidden_dim_bilstm\n",
    "        logits = self.dense(x) #batchsize, max_seq_len, num_labels\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#I need to convert the indices my model will predict back to its actual class\n",
    "#this is what the function below does\n",
    "#for eg 0->'O'\n",
    "#       1->'B-location'\n",
    "#this is done for the predictions\n",
    "def idx_to_label(predictions, idx2Label):\n",
    "    \n",
    "    #store all the predictions in this list\n",
    "    label_pred = []\n",
    "    \n",
    "    for i,pred_batch in enumerate(predictions):\n",
    "        \n",
    "        for sentence in pred_batch:\n",
    "            #store the predictions for a sentence in this\n",
    "            sentence_pred = []\n",
    "            for i in sentence:\n",
    "                sentence_pred.append(idx2Label[i])\n",
    "            #at the end of the sentence,append the predictions\n",
    "            label_pred.append(sentence_pred)\n",
    "                    \n",
    "    return label_pred\n",
    "\n",
    "#load the model weights to test\n",
    "test_model =  NERmodel(max_seq_len=max_seq_len, embed_input_dim=len(word2Idx), embed_output_dim=EMBEDDING_DIM, weights=[embedding_matrix], num_labels=num_labels)\n",
    "test_model.load_weights(f\"../models/best-lstm-v8\")\n",
    "\n",
    "#list to store the prediction labels\n",
    "pred_labels = []\n",
    "\n",
    "for sentences_batch in test_dataset:\n",
    "    \n",
    "    logits = test_model(sentences_batch)\n",
    "    temp1 = tf.nn.softmax(logits)\n",
    "    preds = tf.argmax(temp1, axis=2)\n",
    "    #true_labels.append(np.asarray(labels_batch))\n",
    "    pred_labels.append(np.asarray(preds))\n",
    "\n",
    "label_pred = idx_to_label(pred_labels, idx2Label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the predictios to a file such that there is one to one correspondence between a line in the test_predictions.txt and \n",
    "#test.txt file\n",
    "ftest = open(\"../data/test_predictions.txt\",\"a\")\n",
    "for i,s in enumerate(test_sentences):\n",
    "\n",
    "    prediction = label_pred[i][0:len(s)]\n",
    "    for pred in prediction:\n",
    "        ftest.write(\"%s\\n\" % pred)\n",
    "    ftest.write(\"\\n\")\n",
    "    #assert len(prediction)==len(s)\n",
    "    #ftest.write\n",
    "ftest.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
