{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the sentences and tags obtained from processing in EDA.ipynb\n",
    "sentences = np.load(os.path.join(DATA_DIR,\"sentences.npy\"),allow_pickle=True)\n",
    "tags = np.load(os.path.join(DATA_DIR,\"tags.npy\"),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get number of samples of sentences we have\n",
    "samples = sentences.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove urls from a sentence\n",
    "def remove_url(sentence,tag):\n",
    "    new_sentence = []\n",
    "    new_tags = []\n",
    "    for i,word in enumerate(sentence):\n",
    "        word = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word)\n",
    "        if word!=\"\":\n",
    "            new_sentence.append(word)\n",
    "            new_tags.append(tag[i])\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return new_sentence,new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Business', 'Alliance', 'will', 'save', 'you', 'time', 'and', 'money', 'and', 'most', 'importantly', ',', 'match', 'you', 'with', 'the', 'right', 'franchise', '.', 'check', 'out', 'at', 'http://ow.ly/2kt2M']\n",
      "['B-corporation', 'I-corporation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['Business', 'Alliance', 'will', 'save', 'you', 'time', 'and', 'money', 'and', 'most', 'importantly', ',', 'match', 'you', 'with', 'the', 'right', 'franchise', '.', 'check', 'out', 'at']\n",
      "['B-corporation', 'I-corporation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "idx=399\n",
    "print(sentences[idx])\n",
    "print(tags[idx])\n",
    "s,t = remove_url(sentences[idx],tags[idx])\n",
    "assert len(s)==len(t)\n",
    "print(s)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove twitter usernames from a sentence\n",
    "def remove_usernames(sentence,tag):\n",
    "    new_sentence = []\n",
    "    new_tags = []\n",
    "    for i,word in enumerate(sentence):\n",
    "        word = re.sub('@[^\\s]+','',word)\n",
    "        if word!=\"\":\n",
    "            new_sentence.append(word)\n",
    "            new_tags.append(tag[i])\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return new_sentence,new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@EnticeMeBaby', 'Well', 'that', 'makes', 'sense', '.', 'When', 'you', 'have', 'real', 'life', 'trauma', ',', 'it', 'can', 'often', 'drain', 'your', 'energy', '&amp;', 'you', 'just', 'withdraw', 'into', 'yourself', '.', 'I', 'do', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['Well', 'that', 'makes', 'sense', '.', 'When', 'you', 'have', 'real', 'life', 'trauma', ',', 'it', 'can', 'often', 'drain', 'your', 'energy', '&amp;', 'you', 'just', 'withdraw', 'into', 'yourself', '.', 'I', 'do', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "idx=190\n",
    "print(sentences[idx])\n",
    "print(tags[idx])\n",
    "s,t = remove_usernames(sentences[idx],tags[idx])\n",
    "assert len(s)==len(t)\n",
    "print(s)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove special chars except letters from a sentence\n",
    "def remove_special_char(sentence,tag):\n",
    "    new_sentence = []\n",
    "    new_tags = []\n",
    "    for i,word in enumerate(sentence):\n",
    "        #word = re.sub('\\W+','', word )\n",
    "        if word.isalpha():\n",
    "            new_sentence.append(word)\n",
    "            new_tags.append(tag[i])\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return new_sentence,new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", 'feelin', 'better', 'than', 'some', 'head', 'on', 'a', 'Sunday', 'afternoon', ',', 'better', 'than', 'a', 'chick', 'that', 'said', 'yes', 'too', 'soon', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['feelin', 'better', 'than', 'some', 'head', 'on', 'a', 'Sunday', 'afternoon', 'better', 'than', 'a', 'chick', 'that', 'said', 'yes', 'too', 'soon']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "idx=1986\n",
    "print(sentences[idx])\n",
    "print(tags[idx])\n",
    "s,t = remove_special_char(sentences[idx],tags[idx])\n",
    "assert len(s)==len(t)\n",
    "print(s)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all words in the sentence to lower case\n",
    "def make_lower(sentence):\n",
    "    return [word.lower() for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", 'feelin', 'better', 'than', 'some', 'head', 'on', 'a', 'Sunday', 'afternoon', ',', 'better', 'than', 'a', 'chick', 'that', 'said', 'yes', 'too', 'soon', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[\"i'm\", 'feelin', 'better', 'than', 'some', 'head', 'on', 'a', 'sunday', 'afternoon', ',', 'better', 'than', 'a', 'chick', 'that', 'said', 'yes', 'too', 'soon', '.']\n"
     ]
    }
   ],
   "source": [
    "idx=1986\n",
    "print(sentences[idx])\n",
    "print(tags[idx])\n",
    "s = make_lower(sentences[idx])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre process each sentence by applying the 4 functions defined above\n",
    "for i in range(samples):\n",
    "    currentSent = sentences[i]\n",
    "    currentTag = tags[i]\n",
    "    \n",
    "    currentSent,currentTag = remove_url(currentSent,currentTag)\n",
    "    currentSent,currentTag = remove_usernames(currentSent,currentTag)\n",
    "    currentSent,currentTag = remove_special_char(currentSent,currentTag)\n",
    "    currentSent = make_lower(currentSent)\n",
    "    \n",
    "    assert len(currentSent)==len(currentTag)\n",
    "    sentences[i]=currentSent\n",
    "    tags[i]=currentTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to get part of speech tagging for each sentence. \n",
    "'''\n",
    "#final processed sentences, tags and POS are stored in these lists\n",
    "processedSentences=[]\n",
    "processedTags=[]\n",
    "POS=[]\n",
    "\n",
    "\n",
    "for i in range(samples):\n",
    "    currentSent = sentences[i]\n",
    "    #from the previous pre-processing if any sentence has turned into an empty string, skip it\n",
    "    if len(currentSent)>0:\n",
    "        doc = nlp(\" \".join(currentSent))\n",
    "        # Generate tokens and pos tags\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        \n",
    "        #since it is possible the POS obtained don't have a one to one correspondence to a word in the sentence, skip these sentences \n",
    "        #the data generated using the above condition was used for training models in traditional-ml.ipynb\n",
    "        #if len(currentSent)!=len(pos):\n",
    "         #   continue\n",
    "        \n",
    "        #assert len(currentSent)==len(pos)==len(tags[i])\n",
    "        \n",
    "        #for the deep learning models, I didn't take the POS in consideration so I used all the sentences and tags data.\n",
    "        POS.append(pos)\n",
    "        processedSentences.append(currentSent)\n",
    "        processedTags.append(tags[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(processedSentences)==len(processedTags)==len(POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['surely', 'mags', 'will', 'line', 'up', 'to', 'feature', 'rt', 'lindsay', 'lohan', 'fails', 'drug', 'test', 'faces', 'more', 'jail', 'time']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-person', 'I-person', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['ADV', 'NOUN', 'AUX', 'VERB', 'ADP', 'PART', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "idx=872\n",
    "assert len(processedSentences[idx])==len(processedTags[idx])==len(POS[idx])\n",
    "print(processedSentences[idx])\n",
    "print(processedTags[idx])\n",
    "print(POS[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(DATA_DIR,\"processed_sents.npy\"),np.array(processedSentences,dtype=np.object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(DATA_DIR,\"processed_tags.npy\"),np.array(processedTags,dtype=np.object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(DATA_DIR,\"pos.npy\"),np.array(POS,dtype=np.object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(DATA_DIR,\"processed_sents_dl.npy\"),np.array(processedSentences,dtype=np.object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(DATA_DIR,\"processed_tags_dl.npy\"),np.array(processedTags,dtype=np.object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(DATA_DIR,\"pos_dl.npy\"),np.array(POS,dtype=np.object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
