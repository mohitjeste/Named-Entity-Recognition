{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.load(os.path.join(DATA_DIR,\"processed_sents_dl.npy\"),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = np.load(os.path.join(DATA_DIR,\"processed_tags_dl.npy\"),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.load(os.path.join(DATA_DIR,\"pos_dl.npy\"),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what I did in this notebook:\n",
    "1. I use the sentences and tags. \n",
    "2. I use data sampling since there a class imbalance problem. I consider only the sentences that contain BIO tags other than 'O'. This ensured I have all tags other than 'O' and my 'O' tags were greatly reduced.\n",
    "3. I create a dictionary mapping every word to an index and every BIO tag to an index because I need to represent a sentence as a list of numbers for the LSTM.\n",
    "4. I map every sentence using the above mapping.\n",
    "5. I perform padding so that every sentence in the training data is of equal length.\n",
    "6. I use glove embeddings to represent every word.\n",
    "7. I use those in my embedding layer and set it to trainable.\n",
    "8. It is followed by a bi-directional LSTM layer.\n",
    "9. It is followed by a dense layer.\n",
    "10. I use sparse categorical cross entropy loss to train the model.\n",
    "11. I use Adam optimizer with an adaptive learning rate that decreases as we train the model for more epochs, to train a model.\n",
    "12. I had to experiment with a decreasing learning rate as we train the model more and also different dimensions of Glove vectors. Finally I found out that the 100 dimensional glove vector provided the best result.\n",
    "\n",
    "\n",
    "Some ideas taken from https://medium.com/analytics-vidhya/ner-tensorflow-2-2-0-9f10dcf5a0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the processed data I only consider sentences that contain BIO tags other than 'O'. \n",
    "#this function will perform sampling as above\n",
    "def sample_O(sentences,tags,Osleft):\n",
    "    #array to store the sampled sentences and its corresponding tags\n",
    "    newSentences=[]\n",
    "    newTags=[]\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        tag=tags[i]\n",
    "        temp=tag.count('O')\n",
    "        #if the number of 'O's in the labels for a sentence==len(tag) then it means it is a sentence containing only 'O' as\n",
    "        #its labels. So skip it.\n",
    "        if temp==len(tag):\n",
    "            continue\n",
    "        else:\n",
    "            newSentences.append(sentence)\n",
    "            newTags.append(tag)\n",
    "            Osleft-=temp\n",
    "            \n",
    "    return newSentences,newTags\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 43672, 'B-location': 516, 'I-location': 230, 'B-group': 245, 'B-corporation': 213, 'B-creative-work': 130, 'B-product': 126, 'B-person': 631, 'I-person': 321, 'I-creative-work': 182, 'I-corporation': 40, 'I-group': 141, 'I-product': 156}\n",
      "2931\n"
     ]
    }
   ],
   "source": [
    "#function to count the number of labels of each type\n",
    "def count_labels(tags):\n",
    "    count={}\n",
    "    for tag in tags:\n",
    "        for word_label in tag:\n",
    "            try:\n",
    "                count[word_label]+=1\n",
    "            except:\n",
    "                count[word_label]=1\n",
    "                \n",
    "    return count\n",
    "\n",
    "count = count_labels(tags)\n",
    "print(count)\n",
    "\n",
    "#variable to count the number of labels other than 'O'\n",
    "otherThanO=0\n",
    "\n",
    "for k,v in count.items():\n",
    "    otherThanO+=v\n",
    "    \n",
    "otherThanO-=count['O']\n",
    "\n",
    "print(otherThanO)\n",
    "#sample sentences such that  only the sentences that contain BIO tags other than 'O' are considered. \n",
    "#This ensured I have all tags other than 'O' and my 'O' tags were greatly reduced.\n",
    "sentences,tags=sample_O(sentences,tags,otherThanO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 15217, 'B-location': 516, 'I-location': 230, 'B-group': 245, 'B-corporation': 213, 'B-creative-work': 130, 'B-product': 126, 'B-person': 631, 'I-person': 321, 'I-creative-work': 182, 'I-corporation': 40, 'I-group': 141, 'I-product': 156}\n"
     ]
    }
   ],
   "source": [
    "#finally we have the following in our sampled data\n",
    "print(count_labels(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sentences)==len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number of sentences we have\n",
    "samples = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all unique BIO labels and words\n",
    "\n",
    "#set to store all unique labels\n",
    "labelSet = set()\n",
    "\n",
    "#set to store all unique words\n",
    "wordSet = set()\n",
    "\n",
    "#iterate over all samples and store unique words and BIO labels\n",
    "for i in range(samples):\n",
    "    \n",
    "    sentence,tag=sentences[i],tags[i]\n",
    "    \n",
    "    assert len(sentence)==len(tag)\n",
    "    \n",
    "    for i,word in enumerate(sentence):\n",
    "        labelSet.add(tag[i])\n",
    "        wordSet.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a word to idx and label to idx mapping for the LSTM I implemented\n",
    " Assign index 0 for ‘PADDING_TOKEN’ and 1 for ‘UNKNOWN_TOKEN’. \n",
    " ‘PADDING_TOKEN’ is used for token at the end of a sentence when one batch has sentences of unequal lengths. \n",
    " ‘UNKNOWN_TOKEN’ is used to represent any word that is not present in the vocabulary.\n",
    "'''\n",
    "\n",
    "# Sort the set by length ensure '0' is assigned to O\n",
    "sorted_labels = sorted(list(labelSet), key=len)\n",
    "\n",
    "# Create mapping for labels\n",
    "label2Idx = {}\n",
    "\n",
    "for label in sorted_labels:\n",
    "    label2Idx[label] = len(label2Idx)\n",
    "\n",
    "#reverse mapping from idx to label\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "\n",
    "# Create mapping for words\n",
    "word2Idx = {}\n",
    "\n",
    "\n",
    "#assing the PADDING_TOKEN and UNKNOWN_TOKEN first\n",
    "if len(word2Idx) == 0:\n",
    "    word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "    word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "    \n",
    "for word in wordSet:\n",
    "    word2Idx[word] = len(word2Idx)\n",
    "    \n",
    "#reverse mapping from word to idx\n",
    "idx2Word = {v: k for k, v in word2Idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/word2Idx.pickle', 'wb') as handle:\n",
    "    pickle.dump(word2Idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../data/idx2Word.pickle', 'wb') as handle:\n",
    "    pickle.dump(idx2Word, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../data/label2Idx.pickle', 'wb') as handle:\n",
    "    pickle.dump(label2Idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../data/idx2Label.pickle', 'wb') as handle:\n",
    "    pickle.dump(idx2Label, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to map the sentence and tag to their correspond ids from the dictionaries created above\n",
    "def map_to_idx(sentence, tag):\n",
    "    sentenceToIdx=[]\n",
    "    labelsToIdx=[]\n",
    "    \n",
    "    for i,word in enumerate(sentence):\n",
    "        if word in word2Idx:\n",
    "            wordIdx = word2Idx[word]\n",
    "        elif word.lower() in word2Idx:\n",
    "            wordIdx = word2Idx[word.lower()]\n",
    "        else:\n",
    "            wordIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "\n",
    "        sentenceToIdx.append(wordIdx)\n",
    "        labelsToIdx.append(label2Idx[tag[i]])\n",
    "        \n",
    "    return sentenceToIdx, labelsToIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataset by mapping words and tags to the ids\n",
    "X = []\n",
    "y = []\n",
    "for i in range(samples):\n",
    "    \n",
    "    sentence,tag=sentences[i],tags[i]\n",
    "    sentenceToIdx, labelsToIdx = map_to_idx(sentence, tag)\n",
    "    X.append(sentenceToIdx)\n",
    "    y.append(labelsToIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot sentence by length\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.title('Number of words per sentence')\n",
    "plt.xlabel('Len (sentence)')\n",
    "plt.ylabel('# samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the above graph I decided setting the max sentence length to 25 is a good idea\n",
    "\n",
    "def padding(X,y, max_len, padding='post'):\n",
    "    padded_sentences = pad_sequences(X, max_len,padding='post')\n",
    "    padded_labels = pad_sequences(y, max_len, padding='post')\n",
    "    return padded_sentences, padded_labels\n",
    "\n",
    "\n",
    "X,y = padding(X,y, 25, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dude', 'i', 'can', 'watch', 'jennifers', 'body', 'i', 'know', 'what', 'imma', 'be', 'doing', 'all', 'week', 'xd', 'i', 'love', 'these', 'movies']\n",
      "['O', 'O', 'O', 'O', 'B-creative-work', 'I-creative-work', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['dude', 'i', 'can', 'watch', 'jennifers', 'body', 'i', 'know', 'what', 'imma', 'be', 'doing', 'all', 'week', 'xd', 'i', 'love', 'these', 'movies', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN', 'PADDING_TOKEN']\n",
      "['O', 'O', 'O', 'O', 'B-creative-work', 'I-creative-work', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "#sanity check that we have the done the mapping right and created the right dataset\n",
    "i=100\n",
    "#print(X[i])\n",
    "#print(y[i])\n",
    "print(sentences[i])\n",
    "print(tags[i])\n",
    "\n",
    "print([idx2Word[w] for w in X[i]])\n",
    "print([idx2Label[w] for w in y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(896, 25)\n",
      "(299, 25)\n",
      "(896, 25)\n",
      "(299, 25)\n"
     ]
    }
   ],
   "source": [
    "#(number of sentences,length of each sentence)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=300\n",
    "BATCH = 32\n",
    "max_seq_len = 25\n",
    "num_labels = len(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading glove embeddings\n",
    "embeddings_index = {}\n",
    "f = open(\"../embeddings/glove.6B.300d.txt\", encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.strip().split(' ')\n",
    "    word = values[0] # the first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') #50d vectors   \n",
    "    #representing the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "embedding_matrix = np.zeros((len(word2Idx), EMBEDDING_DIM))\n",
    "# Word embeddings for the tokens\n",
    "\n",
    "\n",
    "for word,i in word2Idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(BATCH)\n",
    "valid_dataset = valid_dataset.batch(BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model which has the following architecture\n",
    "#Embedding->BiLSTM->Dense\n",
    "class NERmodel(tf.keras.Model):\n",
    "    def __init__(self, max_seq_len, embed_input_dim, embed_output_dim, num_labels, weights):\n",
    "\n",
    "        super(NERmodel, self).__init__() \n",
    "        self.embedding = layers.Embedding(input_dim=embed_input_dim,\n",
    "                                          output_dim=embed_output_dim, weights=weights,\n",
    "                                          input_length=max_seq_len, trainable=True, mask_zero=True)        \n",
    "\n",
    "        self.bilstm = layers.Bidirectional(layers.LSTM(512,return_sequences=True))\n",
    "        #self.dense = layers.Dense(num_labels)\n",
    "        self.dense = layers.Dense(num_labels)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.embedding(inputs) # batchsize, max_seq_len,embedding_output_dim\n",
    "        x = self.bilstm(x) #batchsize, max_seq_len, hidden_dim_bilstm\n",
    "        logits = self.dense(x) #batchsize, max_seq_len, num_labels\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NERmodel(max_seq_len=max_seq_len,embed_input_dim=len(word2Idx), embed_output_dim=EMBEDDING_DIM, weights=[embedding_matrix], num_labels=num_labels)\n",
    "\n",
    "#define the learning rate schedule, optimizer and loss function\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment in case a previous model training is to be resumed\n",
    "#model.load_weights(f\"../models/best-lstm-v8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training loss 1.4986\n",
      "Validation loss 1.4263\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "Training loss 1.2781\n",
      "Validation loss 1.3153\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Training loss 1.1753\n",
      "Validation loss 1.2663\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Training loss 1.0965\n",
      "Validation loss 1.2367\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Training loss 1.0272\n",
      "Validation loss 1.2308\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "Training loss 0.9760\n",
      "Validation loss 1.2343\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "Training loss 0.9362\n",
      "Validation loss 1.2558\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "Training loss 0.9068\n",
      "Validation loss 1.2249\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "Training loss 0.8745\n",
      "Validation loss 1.2084\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "Training loss 0.8452\n",
      "Validation loss 1.1930\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "Training loss 0.8203\n",
      "Validation loss 1.1912\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "Training loss 0.7986\n",
      "Validation loss 1.1925\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "Training loss 0.7798\n",
      "Validation loss 1.1908\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "Training loss 0.7607\n",
      "Validation loss 1.1831\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "Training loss 0.7412\n",
      "Validation loss 1.1673\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "Training loss 0.7240\n",
      "Validation loss 1.1785\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "Training loss 0.7066\n",
      "Validation loss 1.1451\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "Training loss 0.6890\n",
      "Validation loss 1.1449\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "Training loss 0.6720\n",
      "Validation loss 1.1118\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "Training loss 0.6554\n",
      "Validation loss 1.1142\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "Training loss 0.6390\n",
      "Validation loss 1.0874\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "Training loss 0.6231\n",
      "Validation loss 1.0859\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "Training loss 0.6073\n",
      "Validation loss 1.0644\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "Training loss 0.5920\n",
      "Validation loss 1.0578\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "Training loss 0.5765\n",
      "Validation loss 1.0429\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "Training loss 0.5618\n",
      "Validation loss 1.0344\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "Training loss 0.5470\n",
      "Validation loss 1.0190\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "Training loss 0.5327\n",
      "Validation loss 1.0127\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "Training loss 0.5185\n",
      "Validation loss 0.9943\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "Training loss 0.5045\n",
      "Validation loss 0.9810\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 30\n",
      "Training loss 0.4910\n",
      "Validation loss 0.9847\n",
      "\n",
      "\n",
      "Epoch 31\n",
      "Training loss 0.4777\n",
      "Validation loss 0.9516\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 32\n",
      "Training loss 0.4647\n",
      "Validation loss 0.9630\n",
      "\n",
      "\n",
      "Epoch 33\n",
      "Training loss 0.4520\n",
      "Validation loss 0.9412\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 34\n",
      "Training loss 0.4396\n",
      "Validation loss 0.9375\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 35\n",
      "Training loss 0.4275\n",
      "Validation loss 0.9234\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 36\n",
      "Training loss 0.4157\n",
      "Validation loss 0.9176\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 37\n",
      "Training loss 0.4043\n",
      "Validation loss 0.9056\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 38\n",
      "Training loss 0.3930\n",
      "Validation loss 0.8990\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 39\n",
      "Training loss 0.3821\n",
      "Validation loss 0.8882\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 40\n",
      "Training loss 0.3715\n",
      "Validation loss 0.8808\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 41\n",
      "Training loss 0.3612\n",
      "Validation loss 0.8712\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 42\n",
      "Training loss 0.3510\n",
      "Validation loss 0.8632\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 43\n",
      "Training loss 0.3413\n",
      "Validation loss 0.8548\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 44\n",
      "Training loss 0.3317\n",
      "Validation loss 0.8470\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 45\n",
      "Training loss 0.3224\n",
      "Validation loss 0.8394\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 46\n",
      "Training loss 0.3134\n",
      "Validation loss 0.8321\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 47\n",
      "Training loss 0.3046\n",
      "Validation loss 0.8253\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 48\n",
      "Training loss 0.2961\n",
      "Validation loss 0.8184\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 49\n",
      "Training loss 0.2879\n",
      "Validation loss 0.8122\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 50\n",
      "Training loss 0.2798\n",
      "Validation loss 0.8056\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 51\n",
      "Training loss 0.2721\n",
      "Validation loss 0.8001\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 52\n",
      "Training loss 0.2645\n",
      "Validation loss 0.7935\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 53\n",
      "Training loss 0.2572\n",
      "Validation loss 0.7886\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 54\n",
      "Training loss 0.2501\n",
      "Validation loss 0.7820\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 55\n",
      "Training loss 0.2432\n",
      "Validation loss 0.7779\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 56\n",
      "Training loss 0.2365\n",
      "Validation loss 0.7713\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 57\n",
      "Training loss 0.2301\n",
      "Validation loss 0.7678\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 58\n",
      "Training loss 0.2238\n",
      "Validation loss 0.7613\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 59\n",
      "Training loss 0.2177\n",
      "Validation loss 0.7585\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 60\n",
      "Training loss 0.2119\n",
      "Validation loss 0.7520\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 61\n",
      "Training loss 0.2062\n",
      "Validation loss 0.7498\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 62\n",
      "Training loss 0.2006\n",
      "Validation loss 0.7433\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 63\n",
      "Training loss 0.1953\n",
      "Validation loss 0.7417\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 64\n",
      "Training loss 0.1901\n",
      "Validation loss 0.7354\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 65\n",
      "Training loss 0.1851\n",
      "Validation loss 0.7341\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 66\n",
      "Training loss 0.1802\n",
      "Validation loss 0.7281\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 67\n",
      "Training loss 0.1755\n",
      "Validation loss 0.7271\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 68\n",
      "Training loss 0.1710\n",
      "Validation loss 0.7214\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 69\n",
      "Training loss 0.1665\n",
      "Validation loss 0.7205\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 70\n",
      "Training loss 0.1623\n",
      "Validation loss 0.7153\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 71\n",
      "Training loss 0.1581\n",
      "Validation loss 0.7143\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 72\n",
      "Training loss 0.1541\n",
      "Validation loss 0.7096\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 73\n",
      "Training loss 0.1502\n",
      "Validation loss 0.7084\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 74\n",
      "Training loss 0.1465\n",
      "Validation loss 0.7042\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 75\n",
      "Training loss 0.1428\n",
      "Validation loss 0.7029\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 76\n",
      "Training loss 0.1393\n",
      "Validation loss 0.6992\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 77\n",
      "Training loss 0.1358\n",
      "Validation loss 0.6982\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 78\n",
      "Training loss 0.1325\n",
      "Validation loss 0.6950\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 79\n",
      "Training loss 0.1293\n",
      "Validation loss 0.6942\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 80\n",
      "Training loss 0.1261\n",
      "Validation loss 0.6914\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 81\n",
      "Training loss 0.1231\n",
      "Validation loss 0.6906\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 82\n",
      "Training loss 0.1202\n",
      "Validation loss 0.6881\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 83\n",
      "Training loss 0.1173\n",
      "Validation loss 0.6872\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 84\n",
      "Training loss 0.1145\n",
      "Validation loss 0.6851\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 85\n",
      "Training loss 0.1119\n",
      "Validation loss 0.6840\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 86\n",
      "Training loss 0.1092\n",
      "Validation loss 0.6823\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 87\n",
      "Training loss 0.1067\n",
      "Validation loss 0.6811\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 88\n",
      "Training loss 0.1043\n",
      "Validation loss 0.6797\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 89\n",
      "Training loss 0.1019\n",
      "Validation loss 0.6784\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 90\n",
      "Training loss 0.0996\n",
      "Validation loss 0.6774\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 91\n",
      "Training loss 0.0973\n",
      "Validation loss 0.6759\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 92\n",
      "Training loss 0.0951\n",
      "Validation loss 0.6753\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 93\n",
      "Training loss 0.0930\n",
      "Validation loss 0.6736\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 94\n",
      "Training loss 0.0909\n",
      "Validation loss 0.6732\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 95\n",
      "Training loss 0.0890\n",
      "Validation loss 0.6717\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 96\n",
      "Training loss 0.0870\n",
      "Validation loss 0.6712\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 97\n",
      "Training loss 0.0851\n",
      "Validation loss 0.6702\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 98\n",
      "Training loss 0.0834\n",
      "Validation loss 0.6539\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 99\n",
      "Training loss 0.0897\n",
      "Validation loss 0.4785\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 100\n",
      "Training loss 0.1010\n",
      "Validation loss 0.4558\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 101\n",
      "Training loss 0.0890\n",
      "Validation loss 0.4866\n",
      "\n",
      "\n",
      "Epoch 102\n",
      "Training loss 0.0818\n",
      "Validation loss 0.4815\n",
      "\n",
      "\n",
      "Epoch 103\n",
      "Training loss 0.0779\n",
      "Validation loss 0.5091\n",
      "\n",
      "\n",
      "Epoch 104\n",
      "Training loss 0.0751\n",
      "Validation loss 0.5240\n",
      "\n",
      "\n",
      "Epoch 105\n",
      "Training loss 0.0730\n",
      "Validation loss 0.5044\n",
      "\n",
      "\n",
      "Epoch 106\n",
      "Training loss 0.0709\n",
      "Validation loss 0.5164\n",
      "\n",
      "\n",
      "Epoch 107\n",
      "Training loss 0.0693\n",
      "Validation loss 0.5296\n",
      "\n",
      "\n",
      "Epoch 108\n",
      "Training loss 0.0677\n",
      "Validation loss 0.5387\n",
      "\n",
      "\n",
      "Epoch 109\n",
      "Training loss 0.0663\n",
      "Validation loss 0.5387\n",
      "\n",
      "\n",
      "Epoch 110\n",
      "Training loss 0.0649\n",
      "Validation loss 0.5415\n",
      "\n",
      "\n",
      "Epoch 111\n",
      "Training loss 0.0636\n",
      "Validation loss 0.5450\n",
      "\n",
      "\n",
      "Epoch 112\n",
      "Training loss 0.0623\n",
      "Validation loss 0.5483\n",
      "\n",
      "\n",
      "Epoch 113\n",
      "Training loss 0.0611\n",
      "Validation loss 0.5520\n",
      "\n",
      "\n",
      "Epoch 114\n",
      "Training loss 0.0598\n",
      "Validation loss 0.5524\n",
      "\n",
      "\n",
      "Epoch 115\n",
      "Training loss 0.0587\n",
      "Validation loss 0.5551\n",
      "\n",
      "\n",
      "Epoch 116\n",
      "Training loss 0.0575\n",
      "Validation loss 0.5557\n",
      "\n",
      "\n",
      "Epoch 117\n",
      "Training loss 0.0564\n",
      "Validation loss 0.5578\n",
      "\n",
      "\n",
      "Epoch 118\n",
      "Training loss 0.0553\n",
      "Validation loss 0.5587\n",
      "\n",
      "\n",
      "Epoch 119\n",
      "Training loss 0.0543\n",
      "Validation loss 0.5605\n",
      "\n",
      "\n",
      "Epoch 120\n",
      "Training loss 0.0532\n",
      "Validation loss 0.5614\n",
      "\n",
      "\n",
      "Epoch 121\n",
      "Training loss 0.0522\n",
      "Validation loss 0.5632\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122\n",
      "Training loss 0.0512\n",
      "Validation loss 0.5639\n",
      "\n",
      "\n",
      "Epoch 123\n",
      "Training loss 0.0502\n",
      "Validation loss 0.5656\n",
      "\n",
      "\n",
      "Epoch 124\n",
      "Training loss 0.0493\n",
      "Validation loss 0.5662\n",
      "\n",
      "\n",
      "Epoch 125\n",
      "Training loss 0.0484\n",
      "Validation loss 0.5679\n",
      "\n",
      "\n",
      "Epoch 126\n",
      "Training loss 0.0475\n",
      "Validation loss 0.5683\n",
      "\n",
      "\n",
      "Epoch 127\n",
      "Training loss 0.0466\n",
      "Validation loss 0.5699\n",
      "\n",
      "\n",
      "Epoch 128\n",
      "Training loss 0.0458\n",
      "Validation loss 0.5702\n",
      "\n",
      "\n",
      "Epoch 129\n",
      "Training loss 0.0449\n",
      "Validation loss 0.5719\n",
      "\n",
      "\n",
      "Epoch 130\n",
      "Training loss 0.0441\n",
      "Validation loss 0.5719\n",
      "\n",
      "\n",
      "Epoch 131\n",
      "Training loss 0.0433\n",
      "Validation loss 0.5736\n",
      "\n",
      "\n",
      "Epoch 132\n",
      "Training loss 0.0425\n",
      "Validation loss 0.5736\n",
      "\n",
      "\n",
      "Epoch 133\n",
      "Training loss 0.0418\n",
      "Validation loss 0.5753\n",
      "\n",
      "\n",
      "Epoch 134\n",
      "Training loss 0.0410\n",
      "Validation loss 0.5752\n",
      "\n",
      "\n",
      "Epoch 135\n",
      "Training loss 0.0403\n",
      "Validation loss 0.5769\n",
      "\n",
      "\n",
      "Epoch 136\n",
      "Training loss 0.0396\n",
      "Validation loss 0.5767\n",
      "\n",
      "\n",
      "Epoch 137\n",
      "Training loss 0.0389\n",
      "Validation loss 0.5784\n",
      "\n",
      "\n",
      "Epoch 138\n",
      "Training loss 0.0382\n",
      "Validation loss 0.5781\n",
      "\n",
      "\n",
      "Epoch 139\n",
      "Training loss 0.0376\n",
      "Validation loss 0.5799\n",
      "\n",
      "\n",
      "Epoch 140\n",
      "Training loss 0.0369\n",
      "Validation loss 0.5795\n",
      "\n",
      "\n",
      "Epoch 141\n",
      "Training loss 0.0363\n",
      "Validation loss 0.5813\n",
      "\n",
      "\n",
      "Epoch 142\n",
      "Training loss 0.0357\n",
      "Validation loss 0.5810\n",
      "\n",
      "\n",
      "Epoch 143\n",
      "Training loss 0.0351\n",
      "Validation loss 0.5827\n",
      "\n",
      "\n",
      "Epoch 144\n",
      "Training loss 0.0345\n",
      "Validation loss 0.5824\n",
      "\n",
      "\n",
      "Epoch 145\n",
      "Training loss 0.0339\n",
      "Validation loss 0.5841\n",
      "\n",
      "\n",
      "Epoch 146\n",
      "Training loss 0.0333\n",
      "Validation loss 0.5838\n",
      "\n",
      "\n",
      "Epoch 147\n",
      "Training loss 0.0327\n",
      "Validation loss 0.5854\n",
      "\n",
      "\n",
      "Epoch 148\n",
      "Training loss 0.0322\n",
      "Validation loss 0.5851\n",
      "\n",
      "\n",
      "Epoch 149\n",
      "Training loss 0.0317\n",
      "Validation loss 0.5868\n",
      "\n",
      "\n",
      "Epoch 150\n",
      "Training loss 0.0311\n",
      "Validation loss 0.5865\n",
      "\n",
      "\n",
      "Epoch 151\n",
      "Training loss 0.0306\n",
      "Validation loss 0.5881\n",
      "\n",
      "\n",
      "Epoch 152\n",
      "Training loss 0.0301\n",
      "Validation loss 0.5879\n",
      "\n",
      "\n",
      "Epoch 153\n",
      "Training loss 0.0296\n",
      "Validation loss 0.5894\n",
      "\n",
      "\n",
      "Epoch 154\n",
      "Training loss 0.0291\n",
      "Validation loss 0.5892\n",
      "\n",
      "\n",
      "Epoch 155\n",
      "Training loss 0.0287\n",
      "Validation loss 0.5906\n",
      "\n",
      "\n",
      "Epoch 156\n",
      "Training loss 0.0282\n",
      "Validation loss 0.5904\n",
      "\n",
      "\n",
      "Epoch 157\n",
      "Training loss 0.0278\n",
      "Validation loss 0.5917\n",
      "\n",
      "\n",
      "Epoch 158\n",
      "Training loss 0.0273\n",
      "Validation loss 0.5915\n",
      "\n",
      "\n",
      "Epoch 159\n",
      "Training loss 0.0269\n",
      "Validation loss 0.5927\n",
      "\n",
      "\n",
      "Epoch 160\n",
      "Training loss 0.0265\n",
      "Validation loss 0.5926\n",
      "\n",
      "\n",
      "Epoch 161\n",
      "Training loss 0.0260\n",
      "Validation loss 0.5937\n",
      "\n",
      "\n",
      "Epoch 162\n",
      "Training loss 0.0256\n",
      "Validation loss 0.5937\n",
      "\n",
      "\n",
      "Epoch 163\n",
      "Training loss 0.0252\n",
      "Validation loss 0.5950\n",
      "\n",
      "\n",
      "Epoch 164\n",
      "Training loss 0.0248\n",
      "Validation loss 0.5951\n",
      "\n",
      "\n",
      "Epoch 165\n",
      "Training loss 0.0244\n",
      "Validation loss 0.5965\n",
      "\n",
      "\n",
      "Epoch 166\n",
      "Training loss 0.0240\n",
      "Validation loss 0.5966\n",
      "\n",
      "\n",
      "Epoch 167\n",
      "Training loss 0.0237\n",
      "Validation loss 0.5980\n",
      "\n",
      "\n",
      "Epoch 168\n",
      "Training loss 0.0233\n",
      "Validation loss 0.5982\n",
      "\n",
      "\n",
      "Epoch 169\n",
      "Training loss 0.0230\n",
      "Validation loss 0.5996\n",
      "\n",
      "\n",
      "Epoch 170\n",
      "Training loss 0.0226\n",
      "Validation loss 0.5997\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-747fa42f1ac2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m#prescision_list = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msentences_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mtraining_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-747fa42f1ac2>\u001b[0m in \u001b[0;36mtrain_step_fn\u001b[1;34m(sentences_batch, labels_batch)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_grad.py\u001b[0m in \u001b[0;36m_ReverseSequenceGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    789\u001b[0m           \u001b[0mbatch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"batch_dim\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m           \u001b[0mseq_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"seq_dim\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m           seq_lengths=seq_lengths), None\n\u001b[0m\u001b[0;32m    792\u001b[0m   ]\n\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mreverse_sequence\u001b[1;34m(input, seq_lengths, seq_axis, batch_axis, name, seq_dim, batch_dim)\u001b[0m\n\u001b[0;32m   3997\u001b[0m       \u001b[0mseq_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseq_axis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3998\u001b[0m       \u001b[0mbatch_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_axis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3999\u001b[1;33m       name=name)\n\u001b[0m\u001b[0;32m   4000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4001\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreverse_sequence\u001b[1;34m(input, seq_lengths, seq_dim, batch_dim, name)\u001b[0m\n\u001b[0;32m   7756\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ReverseSequence\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7757\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"seq_dim\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"batch_dim\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7758\u001b[1;33m         batch_dim)\n\u001b[0m\u001b[0;32m   7759\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7760\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#function defining one step of gradient descent on a given batch\n",
    "def train_step_fn(sentences_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(sentences_batch)\n",
    "        loss = scce(labels_batch, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(list(zip(grads,model.trainable_variables)))\n",
    "    return loss, logits\n",
    "\n",
    "#evaluate the model on the one batch in the validation dataset\n",
    "def valid_step_fn(sentences_batch, labels_batch):\n",
    "    logits = model(sentences_batch)\n",
    "    loss = scce(labels_batch, logits)\n",
    "    return loss, logits\n",
    "\n",
    "\n",
    "#variable to decide whether to save the model in the current iteration. If the model has the least validation loss uptil that \n",
    "#training step, save it.\n",
    "min_val_loss=float('inf')\n",
    "\n",
    "for epoch in range(200):\n",
    "    \n",
    "    #list to store the training and validation loss for this epoch\n",
    "    training_loss=[]\n",
    "    val_loss = []\n",
    "    \n",
    "    #perform one epoch on the training data\n",
    "    for sentences_batch, labels_batch in train_dataset :\n",
    "        loss, logits = train_step_fn(sentences_batch, labels_batch)\n",
    "        training_loss.append(loss)\n",
    "        \n",
    "    #evaluate the model on the validation set\n",
    "    for sentences_batch, labels_batch in valid_dataset:\n",
    "        loss, logits = valid_step_fn(sentences_batch, labels_batch)\n",
    "        val_loss.append(loss)\n",
    "        \n",
    "    \n",
    "    curr_val_loss = sum(val_loss)/len(val_loss)\n",
    "    print(\"Epoch\",epoch)    \n",
    "    print(\"Training loss %.4f\"%(sum(training_loss)/len(training_loss)))\n",
    "    print(\"Validation loss %.4f\"%(curr_val_loss))\n",
    "    \n",
    "    if curr_val_loss<min_val_loss:\n",
    "        print(\"Saving model...\")\n",
    "        model.save_weights(f\"../models/best-lstm-v9\",save_format='tf')\n",
    "        min_val_loss=curr_val_loss\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while evaluating the model we remove the 'O' class, since it is the dominating class and skews results\n",
    "new_labels = list(label2Idx.keys())\n",
    "new_labels.remove('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHIT\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#in order to use sklearn's function to get the F1 score I need to convert the indices my model will predict back to its actual class\n",
    "#this is what the function below does\n",
    "#for eg 0->'O'\n",
    "#       1->'B-location'\n",
    "#this is done for the predictions and ground truth\n",
    "def idx_to_label(predictions, correct, idx2Label):\n",
    "    \n",
    "    label_pred = []\n",
    "    label_correct = []\n",
    "    \n",
    "    for i,pred_batch in enumerate(predictions):\n",
    "        \n",
    "        true_batch=correct[i]\n",
    "        \n",
    "        for sentence in pred_batch:\n",
    "            for i in sentence:\n",
    "                label_pred.append(idx2Label[i])\n",
    "                \n",
    "        if correct != None:\n",
    "            for sentence in true_batch:\n",
    "                for i in sentence:\n",
    "                    label_correct.append(idx2Label[i])\n",
    "                    \n",
    "    \n",
    "    return label_correct, label_pred\n",
    "\n",
    "#load the model weights to test\n",
    "test_model =  NERmodel(max_seq_len=max_seq_len, embed_input_dim=len(word2Idx), embed_output_dim=EMBEDDING_DIM, weights=[embedding_matrix], num_labels=num_labels)\n",
    "test_model.load_weights(f\"../models/best-lstm-v9\")\n",
    "\n",
    "#list to store the true and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for sentences_batch, labels_batch in valid_dataset:\n",
    "    \n",
    "    logits = test_model(sentences_batch)\n",
    "    temp1 = tf.nn.softmax(logits)\n",
    "    preds = tf.argmax(temp1, axis=2)\n",
    "    true_labels.append(np.asarray(labels_batch))\n",
    "    pred_labels.append(np.asarray(preds))\n",
    "\n",
    "label_correct, label_pred = idx_to_label(pred_labels, true_labels, idx2Label)\n",
    "\n",
    "report = classification_report(label_correct, label_pred, digits=4,labels=new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        B-group     0.3226    0.1562    0.2105        64\n",
      "        I-group     0.3600    0.2308    0.2812        39\n",
      "       I-person     0.6269    0.5122    0.5638        82\n",
      "       B-person     0.6686    0.7386    0.7019       153\n",
      "      I-product     0.6154    0.1818    0.2807        44\n",
      "      B-product     0.6000    0.2432    0.3462        37\n",
      "     I-location     0.4493    0.5254    0.4844        59\n",
      "     B-location     0.7303    0.4710    0.5727       138\n",
      "  I-corporation     0.0000    0.0000    0.0000         8\n",
      "  B-corporation     0.8929    0.4717    0.6173        53\n",
      "I-creative-work     0.1250    0.0222    0.0377        45\n",
      "B-creative-work     0.5000    0.1081    0.1778        37\n",
      "\n",
      "      micro avg     0.6073    0.4177    0.4949       759\n",
      "      macro avg     0.4909    0.3051    0.3562       759\n",
      "   weighted avg     0.5750    0.4177    0.4635       759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
