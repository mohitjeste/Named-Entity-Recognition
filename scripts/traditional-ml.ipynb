{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedSentences = np.load(os.path.join(DATA_DIR,\"processed_sents.npy\"),allow_pickle=True)\n",
    "processedTags = np.load(os.path.join(DATA_DIR,\"processed_tags.npy\"),allow_pickle=True)\n",
    "POS = np.load(os.path.join(DATA_DIR,\"pos.npy\"),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42630\n",
      "42630\n",
      "42630\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "I used the POS tags and words as my features and the BIO tags as my output. I use that to build models like Perceptron, Passive\n",
    "Aggressive classifier, SGD classifier and Multinomial Naive Bayes. For these models I need the data in the a data frame having \n",
    "columns such that each word has 1 POS and 1 BIO tag\n",
    "POS, words, tags\n",
    "which is why I flatten the POS, sentences and tags numpy arrays\n",
    "'''\n",
    "pos_flat = [item for sublist in POS for item in sublist]\n",
    "print(len(pos_flat))\n",
    "\n",
    "sentences_flat = [item for sublist in processedSentences for item in sublist]\n",
    "print(len(sentences_flat))\n",
    "\n",
    "tags_flat = [item for sublist in processedTags for item in sublist]\n",
    "print(len(tags_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {'pos': pos_flat,\n",
    "     'words': sentences_flat,\n",
    "     'tags': tags_flat\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRON</td>\n",
       "      <td>it</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DET</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>view</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADP</td>\n",
       "      <td>from</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADV</td>\n",
       "      <td>where</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42625</th>\n",
       "      <td>AUX</td>\n",
       "      <td>been</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42626</th>\n",
       "      <td>VERB</td>\n",
       "      <td>made</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42627</th>\n",
       "      <td>VERB</td>\n",
       "      <td>sat</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42628</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>jan</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42629</th>\n",
       "      <td>VERB</td>\n",
       "      <td>utc</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42630 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos  words tags\n",
       "0      PRON     it    O\n",
       "1       DET    the    O\n",
       "2      NOUN   view    O\n",
       "3       ADP   from    O\n",
       "4       ADV  where    O\n",
       "...     ...    ...  ...\n",
       "42625   AUX   been    O\n",
       "42626  VERB   made    O\n",
       "42627  VERB    sat    O\n",
       "42628  NOUN    jan    O\n",
       "42629  VERB    utc    O\n",
       "\n",
       "[42630 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (34104, 7696)\n",
      "y_train (34104,)\n",
      "X_val (8526, 7696)\n",
      "y_val (8526,)\n"
     ]
    }
   ],
   "source": [
    "#create the X numpy array which contains the features from POS and words.y are the labels. Split to training and validation.\n",
    "X = data.drop('tags', axis=1)\n",
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(X.to_dict('records'))\n",
    "\n",
    "y = data.tags.values\n",
    "classes = np.unique(y)\n",
    "classes = classes.tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"X_val\",X_val.shape)\n",
    "print(\"y_val\",y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the data to a SGD classifier\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-corporation', 'B-creative-work', 'B-group', 'B-location', 'B-person', 'B-product', 'I-corporation', 'I-creative-work', 'I-group', 'I-location', 'I-person', 'I-product']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_classes = classes.copy()\n",
    "new_classes.remove('O')\n",
    "print(new_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  B-corporation       0.94      0.38      0.54        45\n",
      "B-creative-work       0.00      0.00      0.00        29\n",
      "        B-group       0.00      0.00      0.00        47\n",
      "     B-location       1.00      0.02      0.04        99\n",
      "       B-person       1.00      0.10      0.18       138\n",
      "      B-product       1.00      0.07      0.12        30\n",
      "  I-corporation       0.00      0.00      0.00        14\n",
      "I-creative-work       0.00      0.00      0.00        43\n",
      "        I-group       0.00      0.00      0.00        19\n",
      "     I-location       1.00      0.03      0.05        40\n",
      "       I-person       1.00      0.05      0.10        55\n",
      "      I-product       0.00      0.00      0.00        30\n",
      "              O       0.94      1.00      0.97      7937\n",
      "\n",
      "       accuracy                           0.94      8526\n",
      "      macro avg       0.53      0.13      0.15      8526\n",
      "   weighted avg       0.92      0.94      0.91      8526\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHIT\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#check on how the classifier is doing on all classes\n",
    "print(classification_report(y_pred=sgd.predict(X_val), y_true=y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  B-corporation       0.94      0.38      0.54        45\n",
      "B-creative-work       0.00      0.00      0.00        29\n",
      "        B-group       0.00      0.00      0.00        47\n",
      "     B-location       1.00      0.02      0.04        99\n",
      "       B-person       1.00      0.10      0.18       138\n",
      "      B-product       1.00      0.07      0.12        30\n",
      "  I-corporation       0.00      0.00      0.00        14\n",
      "I-creative-work       0.00      0.00      0.00        43\n",
      "        I-group       0.00      0.00      0.00        19\n",
      "     I-location       1.00      0.03      0.05        40\n",
      "       I-person       1.00      0.05      0.10        55\n",
      "      I-product       0.00      0.00      0.00        30\n",
      "\n",
      "      micro avg       0.97      0.07      0.12       589\n",
      "      macro avg       0.50      0.05      0.09       589\n",
      "   weighted avg       0.69      0.07      0.11       589\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHIT\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#check on how the classifier is doing on all classes except 'O'. This is the true measure of performance.\n",
    "#SGD does very bad having a F1 score of only 0.11\n",
    "print(classification_report(y_pred=sgd.predict(X_val), y_true=y_val, labels=new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the perceptron model\n",
    "import pickle\n",
    "filename = '../models/sgd.sav'\n",
    "pickle.dump(sgd, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 16.91, NNZs: 202, Bias: -2.000000, T: 34104, Avg. loss: 0.006832\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 25.77, NNZs: 274, Bias: -2.000000, T: 68208, Avg. loss: 0.003607\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.27, NNZs: 329, Bias: -2.000000, T: 102312, Avg. loss: 0.002346\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 34.50, NNZs: 357, Bias: -3.000000, T: 136416, Avg. loss: 0.001965\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 36.61, NNZs: 379, Bias: -2.000000, T: 170520, Avg. loss: 0.001965\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 38.39, NNZs: 402, Bias: -3.000000, T: 204624, Avg. loss: 0.001701\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 40.10, NNZs: 415, Bias: -3.000000, T: 238728, Avg. loss: 0.001730\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 41.71, NNZs: 431, Bias: -2.000000, T: 272832, Avg. loss: 0.001877\n",
      "Total training time: 2.86 seconds.\n",
      "Convergence after 8 epochs took 2.86 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 11.58, NNZs: 125, Bias: -3.000000, T: 34104, Avg. loss: 0.006480\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.49, NNZs: 181, Bias: -2.000000, T: 68208, Avg. loss: 0.005337\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 23.49, NNZs: 220, Bias: -3.000000, T: 102312, Avg. loss: 0.004369\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 26.65, NNZs: 240, Bias: -2.000000, T: 136416, Avg. loss: 0.004134\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 29.15, NNZs: 261, Bias: -3.000000, T: 170520, Avg. loss: 0.004252\n",
      "Total training time: 1.86 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 30.92, NNZs: 274, Bias: -3.000000, T: 204624, Avg. loss: 0.004340\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 33.26, NNZs: 290, Bias: -2.000000, T: 238728, Avg. loss: 0.003812\n",
      "Total training time: 2.61 seconds.\n",
      "Convergence after 7 epochs took 2.61 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    5.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 19.29, NNZs: 315, Bias: -2.000000, T: 34104, Avg. loss: 0.011113\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 30.79, NNZs: 452, Bias: -3.000000, T: 68208, Avg. loss: 0.007096\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 39.24, NNZs: 552, Bias: -2.000000, T: 102312, Avg. loss: 0.004193\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 43.66, NNZs: 594, Bias: -3.000000, T: 136416, Avg. loss: 0.003401\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 46.58, NNZs: 623, Bias: -4.000000, T: 170520, Avg. loss: 0.003900\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 49.54, NNZs: 653, Bias: -3.000000, T: 204624, Avg. loss: 0.003812\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 51.48, NNZs: 690, Bias: -3.000000, T: 238728, Avg. loss: 0.003460\n",
      "Total training time: 2.55 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 53.63, NNZs: 713, Bias: -3.000000, T: 272832, Avg. loss: 0.003196\n",
      "Total training time: 2.92 seconds.\n",
      "Convergence after 8 epochs took 2.92 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    8.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 28.50, NNZs: 576, Bias: -2.000000, T: 34104, Avg. loss: 0.021904\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 43.03, NNZs: 796, Bias: -3.000000, T: 68208, Avg. loss: 0.012872\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 52.29, NNZs: 937, Bias: -3.000000, T: 102312, Avg. loss: 0.008709\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 57.88, NNZs: 1028, Bias: -3.000000, T: 136416, Avg. loss: 0.007858\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 61.81, NNZs: 1094, Bias: -4.000000, T: 170520, Avg. loss: 0.007301\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 65.68, NNZs: 1154, Bias: -3.000000, T: 204624, Avg. loss: 0.006627\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 68.80, NNZs: 1201, Bias: -3.000000, T: 238728, Avg. loss: 0.007477\n",
      "Total training time: 2.61 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 71.62, NNZs: 1243, Bias: -2.000000, T: 272832, Avg. loss: 0.006803\n",
      "Total training time: 3.00 seconds.\n",
      "Convergence after 8 epochs took 3.00 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   11.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 31.11, NNZs: 682, Bias: -2.000000, T: 34104, Avg. loss: 0.021992\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 46.48, NNZs: 945, Bias: -2.000000, T: 68208, Avg. loss: 0.013048\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 57.69, NNZs: 1145, Bias: -2.000000, T: 102312, Avg. loss: 0.006011\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 63.44, NNZs: 1238, Bias: -2.000000, T: 136416, Avg. loss: 0.005014\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 67.36, NNZs: 1308, Bias: -2.000000, T: 170520, Avg. loss: 0.005043\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 71.08, NNZs: 1372, Bias: -2.000000, T: 204624, Avg. loss: 0.004692\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 74.50, NNZs: 1428, Bias: -2.000000, T: 238728, Avg. loss: 0.004750\n",
      "Total training time: 2.45 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 77.19, NNZs: 1466, Bias: -2.000000, T: 272832, Avg. loss: 0.004750\n",
      "Total training time: 2.81 seconds.\n",
      "Convergence after 8 epochs took 2.81 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   14.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 14.14, NNZs: 150, Bias: -2.000000, T: 34104, Avg. loss: 0.004838\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 21.54, NNZs: 206, Bias: -2.000000, T: 68208, Avg. loss: 0.003079\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 26.31, NNZs: 248, Bias: -2.000000, T: 102312, Avg. loss: 0.001847\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 29.15, NNZs: 266, Bias: -3.000000, T: 136416, Avg. loss: 0.001701\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 31.21, NNZs: 281, Bias: -2.000000, T: 170520, Avg. loss: 0.001730\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 33.02, NNZs: 289, Bias: -3.000000, T: 204624, Avg. loss: 0.001437\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 34.21, NNZs: 302, Bias: -3.000000, T: 238728, Avg. loss: 0.001554\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 35.75, NNZs: 312, Bias: -3.000000, T: 272832, Avg. loss: 0.001407\n",
      "Total training time: 2.77 seconds.\n",
      "Convergence after 8 epochs took 2.77 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   16.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 7.35, NNZs: 51, Bias: -2.000000, T: 34104, Avg. loss: 0.001525\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.77, NNZs: 63, Bias: -2.000000, T: 68208, Avg. loss: 0.001290\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.71, NNZs: 74, Bias: -2.000000, T: 102312, Avg. loss: 0.000880\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.90, NNZs: 80, Bias: -3.000000, T: 136416, Avg. loss: 0.001056\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 16.49, NNZs: 87, Bias: -2.000000, T: 170520, Avg. loss: 0.000880\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 18.00, NNZs: 96, Bias: -1.000000, T: 204624, Avg. loss: 0.000821\n",
      "Total training time: 2.12 seconds.\n",
      "Convergence after 6 epochs took 2.12 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   19.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 13.42, NNZs: 162, Bias: -3.000000, T: 34104, Avg. loss: 0.008973\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.92, NNZs: 202, Bias: -3.000000, T: 68208, Avg. loss: 0.008914\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 23.19, NNZs: 244, Bias: -4.000000, T: 102312, Avg. loss: 0.008210\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 26.80, NNZs: 265, Bias: -3.000000, T: 136416, Avg. loss: 0.008152\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 28.98, NNZs: 295, Bias: -5.000000, T: 170520, Avg. loss: 0.008181\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 31.65, NNZs: 315, Bias: -3.000000, T: 204624, Avg. loss: 0.007888\n",
      "Total training time: 2.12 seconds.\n",
      "Convergence after 6 epochs took 2.12 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   21.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 13.71, NNZs: 153, Bias: -3.000000, T: 34104, Avg. loss: 0.007272\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 19.80, NNZs: 199, Bias: -2.000000, T: 68208, Avg. loss: 0.006451\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 25.10, NNZs: 244, Bias: -2.000000, T: 102312, Avg. loss: 0.005307\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 29.22, NNZs: 280, Bias: -2.000000, T: 136416, Avg. loss: 0.004545\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 31.27, NNZs: 308, Bias: -4.000000, T: 170520, Avg. loss: 0.005043\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 33.20, NNZs: 312, Bias: -2.000000, T: 204624, Avg. loss: 0.005043\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 34.55, NNZs: 328, Bias: -4.000000, T: 238728, Avg. loss: 0.005161\n",
      "Total training time: 2.51 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 36.30, NNZs: 334, Bias: -4.000000, T: 272832, Avg. loss: 0.004779\n",
      "Total training time: 2.87 seconds.\n",
      "Convergence after 8 epochs took 2.87 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   24.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 18.44, NNZs: 259, Bias: -2.000000, T: 34104, Avg. loss: 0.010703\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 27.78, NNZs: 371, Bias: -3.000000, T: 68208, Avg. loss: 0.007712\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 35.47, NNZs: 447, Bias: -2.000000, T: 102312, Avg. loss: 0.005923\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 39.47, NNZs: 484, Bias: -3.000000, T: 136416, Avg. loss: 0.005337\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 42.38, NNZs: 521, Bias: -3.000000, T: 170520, Avg. loss: 0.005659\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 45.23, NNZs: 546, Bias: -4.000000, T: 204624, Avg. loss: 0.005161\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 48.06, NNZs: 585, Bias: -3.000000, T: 238728, Avg. loss: 0.004633\n",
      "Total training time: 2.54 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 49.84, NNZs: 601, Bias: -3.000000, T: 272832, Avg. loss: 0.005366\n",
      "Total training time: 2.91 seconds.\n",
      "Convergence after 8 epochs took 2.91 seconds\n",
      "-- Epoch 1\n",
      "Norm: 23.24, NNZs: 399, Bias: -2.000000, T: 34104, Avg. loss: 0.013224\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 34.99, NNZs: 554, Bias: -3.000000, T: 68208, Avg. loss: 0.008064\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 43.52, NNZs: 661, Bias: -2.000000, T: 102312, Avg. loss: 0.004633\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 48.68, NNZs: 734, Bias: -2.000000, T: 136416, Avg. loss: 0.004105\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 52.21, NNZs: 777, Bias: -3.000000, T: 170520, Avg. loss: 0.003636\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 54.92, NNZs: 826, Bias: -3.000000, T: 204624, Avg. loss: 0.003401\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 57.46, NNZs: 850, Bias: -2.000000, T: 238728, Avg. loss: 0.003313\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 59.63, NNZs: 876, Bias: -3.000000, T: 272832, Avg. loss: 0.003548\n",
      "Total training time: 2.84 seconds.\n",
      "Convergence after 8 epochs took 2.84 seconds\n",
      "-- Epoch 1\n",
      "Norm: 15.03, NNZs: 189, Bias: -2.000000, T: 34104, Avg. loss: 0.007624\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.89, NNZs: 266, Bias: -2.000000, T: 68208, Avg. loss: 0.005659\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 29.46, NNZs: 331, Bias: -2.000000, T: 102312, Avg. loss: 0.004545\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 32.95, NNZs: 361, Bias: -3.000000, T: 136416, Avg. loss: 0.003900\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 35.75, NNZs: 380, Bias: -3.000000, T: 170520, Avg. loss: 0.004046\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 37.66, NNZs: 404, Bias: -2.000000, T: 204624, Avg. loss: 0.004134\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 39.72, NNZs: 420, Bias: -3.000000, T: 238728, Avg. loss: 0.003958\n",
      "Total training time: 2.54 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 41.21, NNZs: 442, Bias: -3.000000, T: 272832, Avg. loss: 0.003871\n",
      "Total training time: 2.90 seconds.\n",
      "Convergence after 8 epochs took 2.90 seconds\n",
      "-- Epoch 1\n",
      "Norm: 62.31, NNZs: 2590, Bias: 1.000000, T: 34104, Avg. loss: 0.110427\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 92.46, NNZs: 3490, Bias: 1.000000, T: 68208, Avg. loss: 0.064157\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 110.58, NNZs: 4037, Bias: 2.000000, T: 102312, Avg. loss: 0.048264\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 122.94, NNZs: 4356, Bias: 1.000000, T: 136416, Avg. loss: 0.041696\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 132.14, NNZs: 4621, Bias: 1.000000, T: 170520, Avg. loss: 0.039937\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 139.67, NNZs: 4802, Bias: 2.000000, T: 204624, Avg. loss: 0.038940\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 145.77, NNZs: 4934, Bias: 1.000000, T: 238728, Avg. loss: 0.039497\n",
      "Total training time: 2.73 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 150.71, NNZs: 5061, Bias: 2.000000, T: 272832, Avg. loss: 0.039321\n",
      "Total training time: 3.09 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 155.90, NNZs: 5147, Bias: 3.000000, T: 306936, Avg. loss: 0.038793\n",
      "Total training time: 3.43 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 159.97, NNZs: 5226, Bias: 2.000000, T: 341040, Avg. loss: 0.039937\n",
      "Total training time: 3.78 seconds.\n",
      "Convergence after 10 epochs took 3.78 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:   36.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(max_iter=50, verbose=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the data to a Perceptron\n",
    "per = Perceptron(verbose=10, max_iter=50)\n",
    "per.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  B-corporation       0.69      0.53      0.60        45\n",
      "B-creative-work       0.43      0.10      0.17        29\n",
      "        B-group       0.69      0.19      0.30        47\n",
      "     B-location       0.44      0.33      0.38        99\n",
      "       B-person       0.67      0.39      0.49       138\n",
      "      B-product       0.70      0.23      0.35        30\n",
      "  I-corporation       0.12      0.14      0.13        14\n",
      "I-creative-work       0.33      0.05      0.08        43\n",
      "        I-group       0.33      0.05      0.09        19\n",
      "     I-location       0.62      0.20      0.30        40\n",
      "       I-person       0.53      0.16      0.25        55\n",
      "      I-product       0.47      0.27      0.34        30\n",
      "\n",
      "      micro avg       0.54      0.27      0.36       589\n",
      "      macro avg       0.50      0.22      0.29       589\n",
      "   weighted avg       0.55      0.27      0.35       589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The perceptron does better than SGD but still not the best possible. Its has an F1 score of 0.35\n",
    "print(classification_report(y_pred=per.predict(X_val), y_true=y_val, labels=new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the perceptron model\n",
    "import pickle\n",
    "filename = '../models/perceptron.sav'\n",
    "pickle.dump(per, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the data to Multinomial Naive bayes\n",
    "nb = MultinomialNB(alpha=0.01)\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  B-corporation       0.75      0.53      0.62        45\n",
      "B-creative-work       0.33      0.03      0.06        29\n",
      "        B-group       0.67      0.21      0.32        47\n",
      "     B-location       0.19      0.37      0.25        99\n",
      "       B-person       0.71      0.37      0.49       138\n",
      "      B-product       0.70      0.23      0.35        30\n",
      "  I-corporation       0.00      0.00      0.00        14\n",
      "I-creative-work       0.50      0.05      0.09        43\n",
      "        I-group       0.11      0.05      0.07        19\n",
      "     I-location       0.29      0.28      0.28        40\n",
      "       I-person       0.05      0.65      0.10        55\n",
      "      I-product       0.38      0.17      0.23        30\n",
      "\n",
      "      micro avg       0.18      0.31      0.23       589\n",
      "      macro avg       0.39      0.25      0.24       589\n",
      "   weighted avg       0.45      0.31      0.30       589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#naive bayes does poor on f1 score for all classes except 'O'\n",
    "print(classification_report(y_pred=nb.predict(X_val), y_true=y_val, labels = new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the perceptron model\n",
    "import pickle\n",
    "filename = '../models/nb.sav'\n",
    "pickle.dump(nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the data to the passive aggresive classifier\n",
    "pa =PassiveAggressiveClassifier()\n",
    "pa.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  B-corporation       0.74      0.51      0.61        45\n",
      "B-creative-work       0.33      0.07      0.11        29\n",
      "        B-group       0.39      0.26      0.31        47\n",
      "     B-location       0.56      0.33      0.42        99\n",
      "       B-person       0.67      0.38      0.49       138\n",
      "      B-product       0.25      0.27      0.26        30\n",
      "  I-corporation       0.00      0.00      0.00        14\n",
      "I-creative-work       0.38      0.07      0.12        43\n",
      "        I-group       0.12      0.05      0.07        19\n",
      "     I-location       0.50      0.12      0.20        40\n",
      "       I-person       0.58      0.13      0.21        55\n",
      "      I-product       0.29      0.13      0.18        30\n",
      "\n",
      "      micro avg       0.52      0.26      0.34       589\n",
      "      macro avg       0.40      0.19      0.25       589\n",
      "   weighted avg       0.50      0.26      0.33       589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this too does as good as the perceptron with a F1 score of 0.34 on all classes except 'O'\n",
    "print(classification_report(y_pred=pa.predict(X_val), y_true=y_val, labels=new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this model\n",
    "import pickle\n",
    "filename = '../pass-agg-cls.sav'\n",
    "pickle.dump(per, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
